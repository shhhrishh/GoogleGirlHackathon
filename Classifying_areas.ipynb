{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9CbsCJwMIW2"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7cAfqFkqXO3",
        "outputId": "656ac11d-0a4a-469d-9e8f-da7b1d73bde5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkscrGT1z2Wn",
        "outputId": "8b3b7cae-92bd-4cbb-b1fe-6720dfd97d60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of the root directory:\n",
            "['Classroom', '211020451_assignment.zip', '211020451_ShristiTiwari.zip', 'Shristi_211020451 (1).pdf', '211020451ass.zip', 'akriti.211010209.zip', 'Shristi_211020451.pdf', '211020451_ass3.pdf', '16431025075258118902503112517639.jpg', '16431027455436902428119220904452.jpg', '211020451_ass4.pdf', '211020451_Shristi_ass3 (1).zip', '211020451_Shristi_ass3.zip', '211020451_ass3 (1).zip', '211020451_ass3.zip', '211020451-ass4.pdf', '211020451_shristi_dldass1.pdf', '211020451_report.pdf', 'Pointers.pdf', '211020451_ass5(C).pdf', 'File-Programs-25-02-2022-updated.docx', 'ques.jpg', 'Interview ', '211020451_ass6.pdf', 'Ass2DLD.pdf', 'Document-Shristi Tiwari.pdf', 'Document.gdoc', 'Document-Shristi Tiwari.gdoc', 'ques1DA.pdf', 'ques2DA.pdf', 'ques3DA.pdf', 'ques5DA.pdf', 'ques6DA.pdf', 'ques4DA.pdf', 'Untitled document (9).gdoc', 'Untitled document (8).gdoc', 'Untitled document (7).gdoc', 'Untitled document (6).gdoc', 'Screenshot_2022-05-17-14-45-06-44_6012fa4d4ddec268fc5c7112cbb265e7 (1).jpg', 'Screenshot_2022-05-17-14-45-06-44_6012fa4d4ddec268fc5c7112cbb265e7.jpg', 'Colab Notebooks', 'DARPAN.pptx', '211020451_Shristi.pptx', 'IMG_20220617_182446.jpg', '10. Control Strutures.gdoc', '1-Introduction- ML vs Conventional deteministic etc (1).gdoc', '1-Introduction- ML vs Conventional deteministic etc.gdoc', 'IMG_20220802_193558 (1).jpg', 'IMG_20220802_193558.jpg', 'FashionShow.gdoc', 'Untitled document (5).gdoc', 'Name - The New Vogue.gdoc', 'POA of Event on Independence Day.gdoc', 'Fashion Show Rule Book.gdoc', 'Sponsors list(Fashion Show):.gdoc', 'Flash cards.gslides', 'Fashion_Show_marketing.gslides', '1. Introduction.gdoc', 'IMG_20220926_214801.jpg', 'shristi.jpg', '211020451_caseStudy (1).gdoc', 'oracle sql.pptx', 'Untitled spreadsheet (3).gsheet', 'Untitled form (File responses)', 'IMG_20221030_161923.jpg', 'IMG_20221030_162104.jpg', 'fashion_background.gslides', 'Untitled.ipynb', 'Linear_regression.ipynb', 'SHRISTI TIWARI, 211020451, DSAI.gdoc', 'SHRISTI_CSLAB4.gdoc', 'Shristi_211020451_CS.gdoc', 'exp.2 AM and DSBSC Modulations.gdoc', 'IMG_20221203_131014.jpg', '211020451_caseStudy.gdoc', 'Club Dé Théâtre.gform', '1_Lab 1.docx', 'disrupt.pptx', 'Untitled document (4).gdoc', 'Disrupt_data.gsheet', 'robotics_assign_211020451.gdoc', 'robotics1_211020451.docx', '211020451_Ass2.gdoc', '211020451_Rob_Ass2.docx', '211020451 Q1.gdoc', '211020451, Q2.gdoc', '211020451 Q3.gdoc', '211020451_Q1_Rob.docx', '211020451_Q3_Robo.docx', '211020451_q2_Rob.docx', 'Minor_4th (1).gslides', 'Minor_4th.gslides', 'IIITNR_PPT_format.pdf', 'IIITNR_PPT_format.gdoc', 'WhatsApp Image 2023-04-01 at 17.17.12.jpeg', '211020451_Shristi_Tiwari.pdf', 'Untitled document.pdf', 'Untitled document (3).gdoc', 'video_20230424_171526.mp4', 'Anish Narayan.mp4', 'Karthik A.mp4', 'Jaydeep Singh.mp4', 'Bhumika.mp4', 'Harshit.mp4', 'Saumya Ranjan.mp4', 'Aditya.mp4', 'Akhil.mp4', 'Ritwik.mp4', 'Sai Sriram.mp4', 'PK Sinha Director.mp4', 'Nidhi vaishnav.mp4', 'Ramakrishna Bandi.mp4', 'Shailesh Khapre.mp4', 'Ramadevi Bandi.mp4', 'Gautam S.mp4', 'Vinayak.mp4', 'PP Paltani.mp4', 'Maifuz Ali.mp4', 'Kishan.mp4', 'Vandit.mp4', 'Satyanarayan Volala.mp4', 'Aakash.mp4', 'Shivani.mp4', 'Chinmai.mp4', 'Abdul.mp4', 'Anirudh.mp4', 'Untitled presentation (1).gslides', 'WhatsApp Video 2023-05-03 at 04.09.15.mp4', 'WhatsApp Video 2023-05-03 at 10.47.37.mp4', 'WhatsApp Video 2023-05-03 at 10.56.06.mp4', 'Control System Final.gslides', 'Untitled spreadsheet (2).gsheet', 'Untitled document (2).gdoc', 'Untitled presentation.gslides', 'minor_4.gslides', 'MLA_model_building_testing.ipynb', 'MLA_project_Pre_process_.ipynb', 'ShristiResume..pdf', 'Final_Minor_4th.gslides', 'Prototyping presentation (2).gslides', 'Case study (1).gslides', 'Prototyping presentation (1).gslides', 'Case study.gslides', 'Prototyping presentation.gslides', 'Copy of D75_5427.JPG', 'Example_report.gdoc', 'Fraud Detection', 'Untitled document (1).gdoc', 'Deep Learning Lab 6.ipynb', 'Untitled spreadsheet (1).gsheet', 'Untitled spreadsheet.gsheet', 'Vircos_Financial_Sheets.gsheet', 'Product RCA.gdoc', 'registration details.gsheet', 'Untitled form.gform', 'Fashion Registration (File responses)', 'Fashion Registration.gform', 'hr ques.gdoc', 'Untitled document.gdoc', 'Meritic Internship JD_copy.pdf', 'assignment_dbms3.docx', 'Shristi_resume_new.pdf', 'SOP_Shristi.pdf', 'Lab2_BDTT.pdf', 'Project Report Seniher 5.gdoc', '20240417_230016.jpg', 'val-20240422T103036Z-001', 'train-20240422T103039Z-003', 'Imagery']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# List contents of the root directory in Google Drive\n",
        "root_contents = os.listdir('/content/drive/MyDrive')\n",
        "print(\"Contents of the root directory:\")\n",
        "print(root_contents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxihDDOaypwG"
      },
      "outputs": [],
      "source": [
        "path_to_imagery = '/content/drive/MyDrive/Imagery'\n",
        "train_data_dir = '/content/drive/MyDrive/Imagery/train'\n",
        "val_data_dir = '/content/drive/MyDrive/Imagery/val'\n",
        "test_data_dir = '/content/drive/MyDrive/Imagery/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGSOU_PVyy_a",
        "outputId": "1fbfdc9f-0e58-40e0-d5dc-2a5feab4618f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data Directory: /content/drive/MyDrive/Imagery/train\n",
            "Validation Data Directory: /content/drive/MyDrive/Imagery/val\n",
            "Test Data Directory: /content/drive/MyDrive/Imagery/test\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Data Directory:\", train_data_dir)\n",
        "print(\"Validation Data Directory:\", val_data_dir)\n",
        "print(\"Test Data Directory:\", test_data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eOB12-s5fDq",
        "outputId": "c2434d75-326c-4dbd-b213-e26beb038ede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 475 images belonging to 2 classes.\n",
            "Found 843 images belonging to 2 classes.\n",
            "Found 831 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "img_width, img_height = 150, 150\n",
        "batch_size = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvfhq3DRBVvl",
        "outputId": "426e0b1e-b2f7-4b4b-9db7-994e4dc958d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 475 images belonging to 2 classes.\n",
            "Found 843 images belonging to 2 classes.\n",
            "Found 831 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_width, img_height = 150, 150\n",
        "batch_size = 32\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Normalization for validation and test data (no augmentation)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Define the paths to your data folders\n",
        "train_data_dir = '/content/drive/MyDrive/Imagery/train'\n",
        "validation_data_dir = '/content/drive/MyDrive/Imagery/val'\n",
        "test_data_dir = '/content/drive/MyDrive/Imagery/test'\n",
        "\n",
        "# Load and prepare training data with target segmentation masks\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',  # Specify 'input' for segmentation masks\n",
        "    color_mode='rgb',  # Assuming RGB images\n",
        "    shuffle=True  # Shuffle the data\n",
        ")\n",
        "\n",
        "# Load and prepare validation data with target segmentation masks\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',  # Specify 'input' for segmentation masks\n",
        "    color_mode='rgb',  # Assuming RGB images\n",
        "    shuffle=False  # Do not shuffle validation data\n",
        ")\n",
        "\n",
        "# Load and prepare test data without target data (only input images)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=None,  # No class mode for test data\n",
        "    color_mode='rgb',  # Assuming RGB images\n",
        "    shuffle=False  # Do not shuffle test data\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPYsiIqUpwtO",
        "outputId": "158d5b98-c0a4-4f6d-f193-33b50566eb00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch type: <class 'tuple'>\n",
            "Batch length: 2\n",
            "Batch data shape: (32, 150, 150, 3)\n",
            "Batch labels shape: (32,)\n"
          ]
        }
      ],
      "source": [
        "# Check the structure of the yielded batches from the generator\n",
        "for batch in train_generator:\n",
        "    print('Batch type:', type(batch))\n",
        "    print('Batch length:', len(batch))\n",
        "    print('Batch data shape:', batch[0].shape)\n",
        "    print('Batch labels shape:', batch[1].shape)\n",
        "    break  # Only print the structure of the first batch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8ayHfaYr6N9",
        "outputId": "9bd8be81-a910-46bd-df74-cdc1a233dae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data batch shape: (32, 150, 150, 3)\n",
            "Labels batch shape: (32,)\n"
          ]
        }
      ],
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "    print('Data batch shape:', data_batch.shape)\n",
        "    print('Labels batch shape:', labels_batch.shape)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEQWOzvfUULy",
        "outputId": "08e63a30-e33f-4d14-9a54-b23200e0bd33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 475 images belonging to 2 classes.\n",
            "Found 843 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define image dimensions, batch size, epochs, and steps per epoch\n",
        "num_classes = 2  # For binary classification with 2 classes\n",
        "img_width, img_height = 150, 150\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Load and prepare training data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,  # Directory containing training images\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'  # For binary classification\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    validation_data_dir,  # Directory containing validation images\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'  # For binary classification\n",
        ")\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Binary classification with 2 classes, using softmax\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN4Y375iUWAr",
        "outputId": "3a6c7593-cedd-40c7-bb1c-a7b3a26cef60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9179\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to /content/drive/MyDrive/Imagery/checkpoints/model_checkpoint.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 1038s 73s/step - loss: 0.1896 - accuracy: 0.9179 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "15/15 [==============================] - 236s 16s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.7888e-05 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.7620e-06 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "15/15 [==============================] - 222s 15s/step - loss: 1.7620e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "15/15 [==============================] - 210s 15s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "15/15 [==============================] - 213s 15s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "15/15 [==============================] - 236s 16s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "15/15 [==============================] - 213s 15s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "15/15 [==============================] - 210s 15s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 9: val_accuracy did not improve from 1.00000\n",
            "15/15 [==============================] - 205s 14s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 10: val_accuracy did not improve from 1.00000\n",
            "15/15 [==============================] - 209s 14s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have already defined and compiled your model and data generators\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define image dimensions, batch size, epochs, and steps per epoch\n",
        "num_classes = 2  # Assuming you have 2 classes\n",
        "img_width, img_height = 150, 150\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "steps_per_epoch = len(train_generator)\n",
        "validation_steps = len(validation_generator)\n",
        "checkpoint_path = '/content/drive/MyDrive/Imagery/checkpoints/model_checkpoint.h5'\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    monitor='val_accuracy',  # You can choose the metric to monitor\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model using the generators and the checkpoint callback\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=len(validation_generator),\n",
        "    callbacks=[checkpoint_callback]  # Include the checkpoint callback in the callbacks list\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXczXcANHJQN"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define image dimensions, batch size, epochs, and steps per epoch\n",
        "num_classes = 2  # Assuming you have 2 classes\n",
        "img_width, img_height = 150, 150\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Assuming you have already defined and compiled your model and data generators\n",
        "steps_per_epoch = len(train_generator)\n",
        "validation_steps = len(validation_generator)\n",
        "\n",
        "# Define the path to save model checkpoints in .h5 format\n",
        "checkpoint_path_h5 = '/content/drive/MyDrive/Imagery/checkpoints/model_checkpoint.h5'\n",
        "checkpoint_callback_h5 = ModelCheckpoint(\n",
        "    checkpoint_path_h5,\n",
        "    monitor='val_accuracy',  # You can choose the metric to monitor\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Define the path to save model checkpoints in .ckpt format (TensorFlow format)\n",
        "checkpoint_path_ckpt = '/content/drive/MyDrive/Imagery/checkpoints/model_checkpoint.ckpt'\n",
        "checkpoint_callback_ckpt = ModelCheckpoint(\n",
        "    checkpoint_path_ckpt,\n",
        "    monitor='val_accuracy',  # You can choose the metric to monitor\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    save_weights_only=True,  # Save only the weights in TensorFlow format\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Define other possible options for saving checkpoints, e.g., specifying save_freq\n",
        "checkpoint_path_custom = '/content/drive/MyDrive/Imagery/checkpoints/model_checkpoint_{epoch:02d}.h5'\n",
        "checkpoint_callback_custom = ModelCheckpoint(\n",
        "    checkpoint_path_custom,\n",
        "    monitor='val_accuracy',  # You can choose the metric to monitor\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    save_freq='epoch',  # Save every epoch\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model using the generators and the checkpoint callbacks\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[checkpoint_callback_h5, checkpoint_callback_ckpt, checkpoint_callback_custom]\n",
        "    # Include the checkpoint callbacks in the callbacks list\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_kJ2PtYGLPK"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "num_classes = 2  # For binary classification with 2 classes\n",
        "img_width, img_height = 150, 150\n",
        "# Define the path to the image you want to test\n",
        "image_path = '/content/7143.jpg'\n",
        "\n",
        "# Load the image and preprocess it\n",
        "img = image.load_img(image_path, target_size=(img_width, img_height))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "img_array /= 255.0  # Normalize the pixel values\n",
        "\n",
        "# Make predictions using your trained model\n",
        "predictions = model.predict(img_array)\n",
        "\n",
        "# Get the predicted class label\n",
        "predicted_class = np.argmax(predictions)\n",
        "\n",
        "# Print the predicted class\n",
        "if predicted_class == 0:\n",
        "    print(\"The image belongs to Class 0 (Minor Damage).\")\n",
        "else:\n",
        "    print(\"The image belongs to Class 1 (Major Damage).\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}